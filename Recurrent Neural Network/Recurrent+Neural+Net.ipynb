{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['anna'],\n",
       "       ['emma'],\n",
       "       ['elizabeth'],\n",
       "       ['minnie'],\n",
       "       ['margaret'],\n",
       "       ['ida'],\n",
       "       ['alice'],\n",
       "       ['bertha'],\n",
       "       ['sarah']],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data \n",
    "path = r'E:\\Study n Work\\Neural Networks\\Recurrent Neural Network\\Data\\Names\\NationalNames.csv'\n",
    "data = pd.read_csv(path)\n",
    "data['Name'] = data['Name']\n",
    "data = np.array(data['Name'][:10000]).reshape(-1,1)\n",
    "data = [x.lower() for x in data[:,0]]\n",
    "\n",
    "data = np.array(data).reshape(-1,1)\n",
    "print(data.shape)\n",
    "data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['anna........']\n",
      " ['emma........']\n",
      " ['elizabeth...']\n",
      " ['minnie......']\n",
      " ['margaret....']\n",
      " ['ida.........']\n",
      " ['alice.......']\n",
      " ['bertha......']\n",
      " ['sarah.......']]\n"
     ]
    }
   ],
   "source": [
    "transform_data = np.copy(data)\n",
    "max_length = 0\n",
    "for index in range(len(data)):\n",
    "    max_length = max(max_length,len(data[index,0]))\n",
    "\n",
    "for index in range(len(data)):\n",
    "    length = (max_length - len(data[index,0]))\n",
    "    string = '.'*length\n",
    "    transform_data[index,0] = ''.join([transform_data[index,0],string])\n",
    "    \n",
    "print(transform_data[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 27\n",
      "Vocab = {'r', 'j', 'p', 'k', 'l', 'm', 'z', 'e', 'c', 'v', 'n', 't', 'u', 'q', '.', 'x', 'b', 'g', 's', 'h', 'a', 'y', 'o', 'f', 'i', 'd', 'w'}\n"
     ]
    }
   ],
   "source": [
    "vocab = list()\n",
    "for name in transform_data[:,0]:\n",
    "    vocab.extend(list(name))\n",
    "\n",
    "vocab = set(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocab size : %d\"%len(vocab))\n",
    "print(\"Vocab = {}\".format(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-20, 22-o\n"
     ]
    }
   ],
   "source": [
    "#map char to id and id to chars\n",
    "char_id = dict()\n",
    "id_char = dict()\n",
    "\n",
    "for i,char in enumerate(vocab):\n",
    "    char_id[char] = i\n",
    "    id_char[i] = char\n",
    "\n",
    "print('a-{}, 22-{}'.format(char_id['a'],id_char[22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "batch_size = 20\n",
    "for i in range(len(transform_data)-batch_size+1):\n",
    "    start = i*batch_size\n",
    "    end = start+batch_size\n",
    "    batch_data = transform_data[start:end]\n",
    "    if(len(batch_data)!=batch_size):\n",
    "        break\n",
    "    char_list = []\n",
    "    for k in range(len(batch_data[0][0])):\n",
    "        batch_dataset = np.zeros([batch_size,len(vocab)])\n",
    "        for j in range(batch_size):\n",
    "            name = batch_data[j][0]\n",
    "            char_index = char_id[name[k]]\n",
    "            batch_dataset[j,char_index] = 1.0\n",
    "            \n",
    "        char_list.append(batch_dataset)\n",
    "    train_dataset.append(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_units = 100\n",
    "hidden_units = 256\n",
    "output_units = vocab_size\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.90\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    mean = 0\n",
    "    std = 0.01\n",
    "    \n",
    "    #lstm cell weights\n",
    "    forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    \n",
    "    #hidden to output weights\n",
    "    hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))\n",
    "    \n",
    "    parameters = dict()\n",
    "    parameters['fgw'] = forget_gate_weights\n",
    "    parameters['igw'] = input_gate_weights\n",
    "    parameters['ogw'] = output_gate_weights\n",
    "    parameters['ggw'] = gate_gate_weights\n",
    "    parameters['how'] = hidden_output_weights\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def initialize_V(parameters):\n",
    "    Vfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Vigw = np.zeros(parameters['igw'].shape)\n",
    "    Vogw = np.zeros(parameters['ogw'].shape)\n",
    "    Vggw = np.zeros(parameters['ggw'].shape)\n",
    "    Vhow = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    V = dict()\n",
    "    V['vfgw'] = Vfgw\n",
    "    V['vigw'] = Vigw\n",
    "    V['vogw'] = Vogw\n",
    "    V['vggw'] = Vggw\n",
    "    V['vhow'] = Vhow\n",
    "    return V\n",
    "\n",
    "def initialize_S(parameters):\n",
    "    Sfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Sigw = np.zeros(parameters['igw'].shape)\n",
    "    Sogw = np.zeros(parameters['ogw'].shape)\n",
    "    Sggw = np.zeros(parameters['ggw'].shape)\n",
    "    Show = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    S = dict()\n",
    "    S['sfgw'] = Sfgw\n",
    "    S['sigw'] = Sigw\n",
    "    S['sogw'] = Sogw\n",
    "    S['sggw'] = Sggw\n",
    "    S['show'] = Show\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n",
    "    exp_X = exp_X/exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1-(X**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    \n",
    "    #concat batch data and prev_activation matrix\n",
    "    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)\n",
    "    \n",
    "    #forget gate activation\n",
    "    fa = np.matmul(concat_dataset,fgw)\n",
    "    fa = sigmoid(fa)\n",
    "    \n",
    "    #input gate activation\n",
    "    ia = np.matmul(concat_dataset,igw)\n",
    "    ia = sigmoid(ia)\n",
    "    \n",
    "    #output gate activation\n",
    "    oa = np.matmul(concat_dataset,ogw)\n",
    "    oa = sigmoid(oa)\n",
    "    \n",
    "    #gate gate activation\n",
    "    ga = np.matmul(concat_dataset,ggw)\n",
    "    ga = tanh_activation(ga)\n",
    "    \n",
    "    #new cell memory matrix\n",
    "    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)\n",
    "    \n",
    "    #current activation matrix\n",
    "    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))\n",
    "    \n",
    "    #lets store the activations to be used in back prop\n",
    "    lstm_activations = dict()\n",
    "    lstm_activations['fa'] = fa\n",
    "    lstm_activations['ia'] = ia\n",
    "    lstm_activations['oa'] = oa\n",
    "    lstm_activations['ga'] = ga\n",
    "    \n",
    "    return lstm_activations,cell_memory_matrix,activation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_cell(activation_matrix,parameters):\n",
    "    #get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get outputs \n",
    "    output_matrix = np.matmul(activation_matrix,how)\n",
    "    output_matrix = softmax(output_matrix)\n",
    "    \n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(batch_dataset,embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset,embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(batches,parameters,embeddings):\n",
    "    batch_size = batches[0].shape[0]\n",
    "    \n",
    "    #to store the activations of all the unrollings.\n",
    "    lstm_cache = dict()\n",
    "    activation_cache = dict()\n",
    "    cell_cache = dict()\n",
    "    output_cache = dict()\n",
    "    embedding_cache = dict()\n",
    "    \n",
    "    #initial activation_matrix and cell_matrix\n",
    "    a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    \n",
    "    #store the initial activations in cache\n",
    "    activation_cache['a0'] = a0\n",
    "    cell_cache['c0'] = c0\n",
    "    \n",
    "    for i in range(len(batches)-1):\n",
    "        batch_dataset = batches[i]\n",
    "        batch_dataset = get_embeddings(batch_dataset,embeddings)\n",
    "        embedding_cache['emb'+str(i)] = batch_dataset\n",
    "        \n",
    "        #lstm cell\n",
    "        lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n",
    "        \n",
    "        #output cell\n",
    "        ot = output_cell(at,parameters)\n",
    "        \n",
    "        #store the time 't' activations in cache\n",
    "        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n",
    "        activation_cache['a'+str(i+1)] = at\n",
    "        cell_cache['c' + str(i+1)] = ct\n",
    "        output_cache['o'+str(i+1)] = ot\n",
    "        \n",
    "        #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "        a0 = at\n",
    "        c0 = ct\n",
    "        \n",
    "    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate loss, perplexity and accuracy\n",
    "def cal_loss_accuracy(batch_labels,output_cache):\n",
    "    loss = 0\n",
    "    acc  = 0\n",
    "    prob = 1\n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))\n",
    "        loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)\n",
    "        acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)\n",
    "    \n",
    "    perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size\n",
    "    loss = np.sum(loss)*(-1/batch_size)\n",
    "    acc  = np.sum(acc)/(batch_size)\n",
    "    acc = acc/len(output_cache)\n",
    "    \n",
    "    return perplexity,loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate output cell errors\n",
    "def calculate_output_cell_error(batch_labels,output_cache,parameters):\n",
    "    output_error_cache = dict()\n",
    "    activation_error_cache = dict()\n",
    "    how = parameters['how']\n",
    "    \n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        error_output = pred - labels\n",
    "        error_activation = np.matmul(error_output,how.T)\n",
    "        \n",
    "        output_error_cache['eo'+str(i)] = error_output\n",
    "        activation_error_cache['ea'+str(i)] = error_activation\n",
    "    return output_error_cache,activation_error_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate error for single lstm cell\n",
    "def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):\n",
    "    activation_error = activation_output_error + next_activation_error\n",
    "    \n",
    "    #output gate error\n",
    "    oa = lstm_activation['oa']\n",
    "    eo = np.multiply(activation_error,tanh_activation(cell_activation))\n",
    "    eo = np.multiply(np.multiply(eo,oa),1-oa)\n",
    "    \n",
    "    #cell activation error\n",
    "    cell_error = np.multiply(activation_error,oa)\n",
    "    cell_error = np.multiply(cell_error,tanh_derivative(tanh_activation(cell_activation)))\n",
    "    cell_error += next_cell_error\n",
    "    \n",
    "    #input gate error\n",
    "    ia = lstm_activation['ia']\n",
    "    ga = lstm_activation['ga']\n",
    "    ei = np.multiply(cell_error,ga)\n",
    "    ei = np.multiply(np.multiply(ei,ia),1-ia)\n",
    "    \n",
    "    #gate gate error\n",
    "    eg = np.multiply(cell_error,ia)\n",
    "    eg = np.multiply(eg,tanh_derivative(ga))\n",
    "    \n",
    "    #forget gate error\n",
    "    fa = lstm_activation['fa']\n",
    "    ef = np.multiply(cell_error,prev_cell_activation)\n",
    "    ef = np.multiply(np.multiply(ef,fa),1-fa)\n",
    "    \n",
    "    #prev cell error\n",
    "    prev_cell_error = np.multiply(cell_error,fa)\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ggw = parameters['ggw']\n",
    "    ogw = parameters['ogw']\n",
    "    \n",
    "    #embedding + hidden activation error\n",
    "    embed_activation_error = np.matmul(ef,fgw.T)\n",
    "    embed_activation_error += np.matmul(ei,igw.T)\n",
    "    embed_activation_error += np.matmul(eo,ogw.T)\n",
    "    embed_activation_error += np.matmul(eg,ggw.T)\n",
    "    \n",
    "    input_hidden_units = fgw.shape[0]\n",
    "    hidden_units = fgw.shape[1]\n",
    "    input_units = input_hidden_units - hidden_units\n",
    "    \n",
    "    #prev activation error\n",
    "    prev_activation_error = embed_activation_error[:,input_units:]\n",
    "    \n",
    "    #input error (embedding error)\n",
    "    embed_error = embed_activation_error[:,:input_units]\n",
    "    \n",
    "    #store lstm error\n",
    "    lstm_error = dict()\n",
    "    lstm_error['ef'] = ef\n",
    "    lstm_error['ei'] = ei\n",
    "    lstm_error['eo'] = eo\n",
    "    lstm_error['eg'] = eg\n",
    "    \n",
    "    return prev_activation_error,prev_cell_error,embed_error,lstm_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate output cell derivatives\n",
    "def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):\n",
    "    dhow = np.zeros(parameters['how'].shape)\n",
    "    batch_size = activation_cache['a1'].shape[0]\n",
    "    \n",
    "    for i in range(1,len(output_error_cache)+1):\n",
    "        output_error = output_error_cache['eo' + str(i)]\n",
    "        activation = activation_cache['a'+str(i)]\n",
    "        dhow += np.matmul(activation.T,output_error)/batch_size\n",
    "        \n",
    "    #dhow = dhow/len(output_error_cache)\n",
    "    return dhow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate derivatives for single lstm cell\n",
    "def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):\n",
    "    ef = lstm_error['ef']\n",
    "    ei = lstm_error['ei']\n",
    "    eo = lstm_error['eo']\n",
    "    eg = lstm_error['eg']\n",
    "    \n",
    "    concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)\n",
    "    batch_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    dfgw = np.matmul(concat_matrix.T,ef)/batch_size\n",
    "    digw = np.matmul(concat_matrix.T,ei)/batch_size\n",
    "    dogw = np.matmul(concat_matrix.T,eo)/batch_size\n",
    "    dggw = np.matmul(concat_matrix.T,eg)/batch_size\n",
    "    \n",
    "    derivatives = dict()\n",
    "    derivatives['dfgw'] = dfgw\n",
    "    derivatives['digw'] = digw\n",
    "    derivatives['dogw'] = dogw\n",
    "    derivatives['dggw'] = dggw\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):\n",
    "    #calculate output errors \n",
    "    output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)\n",
    "    \n",
    "    #to store error lstms\n",
    "    lstm_error_cache = dict()\n",
    "    \n",
    "    #to store embeding errors\n",
    "    embedding_error_cache = dict()\n",
    "    \n",
    "    # next activation error \n",
    "    # next cell error\n",
    "    eat = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    ect = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    \n",
    "    #calculate all lstm cell errors\n",
    "    for i in range(len(lstm_cache),0,-1):\n",
    "        pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])\n",
    "        lstm_error_cache['elstm'+str(i)] = le\n",
    "        embedding_error_cache['eemb'+str(i-1)] = ee\n",
    "        eat = pae\n",
    "        ect = pce\n",
    "    \n",
    "    #calculate output cell derivatives\n",
    "    derivatives = dict()\n",
    "    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)\n",
    "    \n",
    "    #calculate lstm cell derivatives\n",
    "    lstm_derivatives = dict()\n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])\n",
    "    \n",
    "    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n",
    "    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    \n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n",
    "        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n",
    "        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n",
    "        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n",
    "    \n",
    "    #derivatives['dfgw'] /= len(lstm_cache)\n",
    "    #derivatives['digw'] /= len(lstm_cache)\n",
    "    #derivatives['dogw'] /= len(lstm_cache)\n",
    "    #derivatives['dggw'] /= len(lstm_cache)\n",
    "    \n",
    "    return derivatives,embedding_error_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adam optimization\n",
    "def update_parameters(parameters,derivatives,V,S,t):\n",
    "    #get derivatives\n",
    "    dfgw = derivatives['dfgw']\n",
    "    digw = derivatives['digw']\n",
    "    dogw = derivatives['dogw']\n",
    "    dggw = derivatives['dggw']\n",
    "    dhow = derivatives['dhow']\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get V parameters\n",
    "    vfgw = V['vfgw']\n",
    "    vigw = V['vigw']\n",
    "    vogw = V['vogw']\n",
    "    vggw = V['vggw']\n",
    "    vhow = V['vhow']\n",
    "    \n",
    "    #get S parameters\n",
    "    sfgw = S['sfgw']\n",
    "    sigw = S['sigw']\n",
    "    sogw = S['sogw']\n",
    "    sggw = S['sggw']\n",
    "    show = S['show']\n",
    "    \n",
    "    vfgw = (beta1*vfgw + (1-beta1)*dfgw)#/(1-(beta1**t))\n",
    "    vigw = (beta1*vigw + (1-beta1)*digw)#/(1-beta1**t)\n",
    "    vogw = (beta1*vogw + (1-beta1)*dogw)#/(1-beta1**t)\n",
    "    vggw = (beta1*vggw + (1-beta1)*dggw)#/(1-beta1**t)\n",
    "    vhow = (beta1*vhow + (1-beta1)*dhow)#/(1-beta1**t)\n",
    "    \n",
    "    sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))#/(1-beta2**t)\n",
    "    sigw = (beta2*sigw + (1-beta2)*(digw**2))#/(1-beta2**t)\n",
    "    sogw = (beta2*sogw + (1-beta2)*(dogw**2))#/(1-beta2**t)\n",
    "    sggw = (beta2*sggw + (1-beta2)*(dggw**2))#/(1-beta2**t)\n",
    "    show = (beta2*show + (1-beta2)*(dhow**2))#/(1-beta2**t)\n",
    "    \n",
    "    fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))\n",
    "    igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))\n",
    "    ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))\n",
    "    ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))\n",
    "    how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))\n",
    "    \n",
    "    parameters['fgw'] = fgw\n",
    "    parameters['igw'] = igw\n",
    "    parameters['ogw'] = ogw\n",
    "    parameters['ggw'] = ggw\n",
    "    parameters['how'] = how\n",
    "    \n",
    "    V['vfgw'] = vfgw \n",
    "    V['vigw'] = vigw \n",
    "    V['vogw'] = vogw \n",
    "    V['vggw'] = vggw\n",
    "    V['vhow'] = vhow\n",
    "    \n",
    "    S['sfgw'] = sfgw \n",
    "    S['sigw'] = sigw \n",
    "    S['sogw'] = sogw \n",
    "    S['sggw'] = sggw\n",
    "    S['show'] = show\n",
    "    \n",
    "    return parameters,V,S    \n",
    "\n",
    "def update_embeddings(embeddings,embedding_error_cache,batch_labels):\n",
    "    embedding_derivatives = np.zeros(embeddings.shape)\n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    for i in range(len(embedding_error_cache)):\n",
    "        embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size\n",
    "    \n",
    "    embeddings = embeddings - learning_rate*embedding_derivatives\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(train_dataset,embeddings,iters=1000,batch_size=20):\n",
    "    parameters = initialize_parameters()\n",
    "    V = initialize_V(parameters)\n",
    "    S = initialize_S(parameters)\n",
    "    embeddings = np.random.normal(0,0.01,(len(vocab),input_units))\n",
    "    \n",
    "    for step in range(iters):\n",
    "        index = step%len(train_dataset)\n",
    "        batches = train_dataset[index]\n",
    "        \n",
    "        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)\n",
    "        perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)\n",
    "        derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)\n",
    "        parameters,V,S = update_parameters(parameters,derivatives,V,S,step)\n",
    "        embeddings = update_embeddings(embeddings,embedding_error_cache,batches)\n",
    "        \n",
    "        if(step%1000==0):\n",
    "            print('Step = {}'.format(step))\n",
    "            print('Loss = {}'.format(round(loss,2)))\n",
    "            print('Perp = {}'.format(round(perplexity,2)))\n",
    "            print('Accu = {}'.format(round(acc*100,2)))\n",
    "            print()\n",
    "    \n",
    "    return embeddings, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0\n",
      "Loss = 47.05\n",
      "Perp = 27.0\n",
      "Accu = 0.91\n",
      "\n",
      "Step = 1000\n",
      "Loss = 16.41\n",
      "Perp = 3.31\n",
      "Accu = 69.55\n",
      "\n",
      "Step = 2000\n",
      "Loss = 14.86\n",
      "Perp = 2.75\n",
      "Accu = 70.0\n",
      "\n",
      "Step = 3000\n",
      "Loss = 12.46\n",
      "Perp = 2.17\n",
      "Accu = 78.18\n",
      "\n",
      "Step = 4000\n",
      "Loss = 10.36\n",
      "Perp = 1.85\n",
      "Accu = 82.73\n",
      "\n",
      "Step = 5000\n",
      "Loss = 9.15\n",
      "Perp = 1.71\n",
      "Accu = 84.55\n",
      "\n",
      "Step = 6000\n",
      "Loss = 8.59\n",
      "Perp = 1.65\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 7000\n",
      "Loss = 8.07\n",
      "Perp = 1.59\n",
      "Accu = 86.82\n",
      "\n",
      "Step = 8000\n",
      "Loss = 7.64\n",
      "Perp = 1.55\n",
      "Accu = 87.73\n",
      "\n",
      "Step = 9000\n",
      "Loss = 7.81\n",
      "Perp = 1.56\n",
      "Accu = 86.82\n",
      "\n",
      "Step = 10000\n",
      "Loss = 7.58\n",
      "Perp = 1.54\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 11000\n",
      "Loss = 7.77\n",
      "Perp = 1.56\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 12000\n",
      "Loss = 7.68\n",
      "Perp = 1.54\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 13000\n",
      "Loss = 7.71\n",
      "Perp = 1.55\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 14000\n",
      "Loss = 7.66\n",
      "Perp = 1.54\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 15000\n",
      "Loss = 7.62\n",
      "Perp = 1.54\n",
      "Accu = 85.91\n",
      "\n",
      "Step = 16000\n",
      "Loss = 7.61\n",
      "Perp = 1.53\n",
      "Accu = 86.36\n",
      "\n",
      "Step = 17000\n",
      "Loss = 7.58\n",
      "Perp = 1.53\n",
      "Accu = 86.36\n",
      "\n",
      "Step = 18000\n",
      "Loss = 7.55\n",
      "Perp = 1.53\n",
      "Accu = 86.36\n",
      "\n",
      "Step = 19000\n",
      "Loss = 7.53\n",
      "Perp = 1.53\n",
      "Accu = 86.36\n",
      "\n",
      "Step = 20000\n",
      "Loss = 7.52\n",
      "Perp = 1.53\n",
      "Accu = 86.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings,parameters = train(train_dataset,embeddings,iters=20001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "def predict(parameters,embeddings,id_char,vocab):\n",
    "    \n",
    "    names = []\n",
    "    for i in range(20):\n",
    "        #initial activation_matrix and cell_matrix\n",
    "        a0 = np.zeros([1,hidden_units],dtype=np.float32)\n",
    "        c0 = np.zeros([1,hidden_units],dtype=np.float32)\n",
    "\n",
    "        #store the initial activations in cache\n",
    "        activation_cache['a0'] = a0\n",
    "        cell_cache['c0'] = c0\n",
    "\n",
    "        name = ''\n",
    "        batch_dataset = np.zeros([1,len(vocab)])\n",
    "        index = np.random.randint(0,27,1)[0]\n",
    "        batch_dataset[0,index] = 1.0\n",
    "        name += id_char[index]\n",
    "        char = id_char[index]\n",
    "        while(char!='.'):\n",
    "            batch_dataset = get_embeddings(batch_dataset,embeddings)\n",
    "\n",
    "            #lstm cell\n",
    "            lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n",
    "\n",
    "            #output cell\n",
    "            ot = output_cell(at,parameters)\n",
    "            pred = np.random.choice(27,1,p=ot[0])[0]\n",
    "            true = np.random.randint(0,1,1)[0]\n",
    "            #pred = np.argmax(ot)\n",
    "                \n",
    "            name += id_char[pred]\n",
    "            char = id_char[pred]\n",
    "            batch_dataset = np.zeros([1,len(vocab)])\n",
    "            batch_dataset[0,pred] = 1.0\n",
    "\n",
    "            #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "            a0 = at\n",
    "            c0 = ct\n",
    "        names.append(name)\n",
    "        \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['richard.',\n",
       " 'natalie.',\n",
       " 'fletcher.',\n",
       " 'oreine.',\n",
       " 'parthena.',\n",
       " 'adolph.',\n",
       " 'ramon.',\n",
       " 'ollie.',\n",
       " 'young.',\n",
       " 'netta.',\n",
       " 'arvella.',\n",
       " 'mark.',\n",
       " 'tina.',\n",
       " 'truman.',\n",
       " 'young.',\n",
       " 'aby.',\n",
       " 'jesus.',\n",
       " 'jess.',\n",
       " 'oscar.',\n",
       " 'helena.']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(parameters,embeddings,id_char,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['donnie.',\n",
       " 'george.',\n",
       " 'vernie.',\n",
       " 'margaret.',\n",
       " 'isabella.',\n",
       " 'isabella.',\n",
       " 'orange.',\n",
       " 'theodore.',\n",
       " 'theodore.',\n",
       " 'johnnie.',\n",
       " 'ursula.',\n",
       " 'clara.',\n",
       " 'harris.',\n",
       " '.',\n",
       " '.',\n",
       " 'george.',\n",
       " 'katheryn.',\n",
       " 'bertie.',\n",
       " 'leonard.',\n",
       " 'leonard.']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(parameters,embeddings,id_char,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_errors(error_activation,next_error_cell,lstm_activation,cell_activation,prev_cell_activation,parameters):\n",
    "    #error_cell_mem 'ct'\n",
    "    ea_t = error_activation\n",
    "    oa = lstm_activation['oa']\n",
    "    ct_der = tanh_derivative(cell_activation)\n",
    "    error_cell = np.multiply(np.multiply(ea_t,oa),ct_der) + next_error_cell\n",
    "    \n",
    "\n",
    "    #error lstm cell\n",
    "    #output gate\n",
    "    oa = lstm_activation['oa']\n",
    "    eo = np.multiply(ea_t,tanh_activation(cell_activation))\n",
    "    eo = np.multiply(np.multiply(eo,oa),1-oa)\n",
    "\n",
    "    #gate gate\n",
    "    ia = lstm_activation['ia']\n",
    "    eg = np.multiply(ia,error_cell)\n",
    "    eg = tanh_derivative(eg)\n",
    "\n",
    "    #input gate\n",
    "    ga = lstm_activation['ga']\n",
    "    ei = np.multiply(ga,error_cell)\n",
    "    ei = np.multiply(np.multiply(ei,ia),1-ia)\n",
    "\n",
    "    #forget gate\n",
    "    prev_cell = prev_cell_activation\n",
    "    fa = lstm_activation['fa']\n",
    "    ef = np.multiply(prev_cell,error_cell)\n",
    "    ef = np.multiply(np.multiply(ef,fa),1-fa)\n",
    "\n",
    "    #error prev cell ct-1\n",
    "    prev_error_cell = np.multiply(fa,error_cell)\n",
    "\n",
    "    #store errors in error_lstm\n",
    "    error_lstm = np.zeros([4,ef.shape[1]])\n",
    "    error_lstm[0,:] = ef[0]\n",
    "    error_lstm[1,:] = ei[0]\n",
    "    error_lstm[2,:] = eo[0]\n",
    "    error_lstm[3,:] = eg[0]\n",
    "    \n",
    "    #error prev activation at-1\n",
    "    fgw = parameters['fgw'][input_units:,:]\n",
    "    igw = parameters['igw'][input_units:,:]\n",
    "    ogw = parameters['ogw'][input_units:,:]\n",
    "    ggw = parameters['ggw'][input_units:,:]\n",
    "    \n",
    "    prev_error_activation = np.matmul(ef,fgw.T)\n",
    "    prev_error_activation += np.matmul(ei,igw.T)\n",
    "    prev_error_activation += np.matmul(eo,ogw.T)\n",
    "    prev_error_activation += np.matmul(eg,ggw.T)\n",
    "    \n",
    "    return error_lstm,prev_error_cell,prev_error_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_activation_matrix(activation_cache):\n",
    "    l = len(activation_cache)-1\n",
    "    activation_matrix = np.zeros([l,activation_cache['a0'].shape[1]])\n",
    "    for i in range(l):\n",
    "        activation_matrix[i] = activation_cache['a'+str(i+1)]\n",
    "    \n",
    "    activation_matrix = activation_matrix.reshape(-1,activation_cache['a0'].shape[1])\n",
    "    return activation_matrix\n",
    "\n",
    "def get_input_activation_matrix(train_name,activation_cache,char_id,char_mat):\n",
    "    l = len(activation_cache)-1\n",
    "    input_activation_matrix = np.zeros([l,char_mat.shape[1]+activation_cache['a0'].shape[1]])\n",
    "    chars = list(train_name)\n",
    "    for i in range(l):\n",
    "        index = char_id[chars[i]]\n",
    "        input_vec = char_mat[index].reshape(-1,char_mat.shape[1])\n",
    "        activation_vec = activation_cache['a'+str(i)]\n",
    "        concat_vec = np.concatenate((input_vec,activation_vec),axis=1)\n",
    "        input_activation_matrix[i] = concat_vec\n",
    "    \n",
    "    input_activation_matrix = input_activation_matrix.reshape(-1,char_mat.shape[1]+activation_cache['a0'].shape[1])\n",
    "    return input_activation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(train_name,parameters,lstm_cache,activation_cache,cell_cache,output,char_id,char_mat):\n",
    "    #lstm parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    \n",
    "    #ihw = parameters['ihw']\n",
    "    #hhw = parameters['hhw']\n",
    "    \n",
    "    #hidden to output weights\n",
    "    how = parameters['how']\n",
    "    \n",
    "    chars = list(train_name)\n",
    "    \n",
    "    derivatives = dict()\n",
    "    derivatives['dfgw'] = np.zeros(shape=fgw.shape)\n",
    "    derivatives['digw'] = np.zeros(shape=igw.shape)\n",
    "    derivatives['dogw'] = np.zeros(shape=ogw.shape)\n",
    "    derivatives['dggw'] = np.zeros(shape=ggw.shape)\n",
    "    \n",
    "    #derivatives['dihw'] = np.zeros(shape=ihw.shape)\n",
    "    #derivatives['dhhw'] = np.zeros(shape=hhw.shape)\n",
    "    derivatives['dhow'] = np.zeros(shape=how.shape)\n",
    "    \n",
    "    error_lstm = dict()\n",
    "    error_output = np.zeros([len(train_name)-1,how.shape[1]])\n",
    "    \n",
    "    for i in range(1,len(train_name)):\n",
    "        #to store errors\n",
    "        #error_activation = dict()\n",
    "        #error_cell = dict()\n",
    "        \n",
    "        \n",
    "        #get lstm activations at time t\n",
    "        lstm_activation = lstm_cache['lstm'+str(i)]\n",
    "        \n",
    "        #get act label and output label;\n",
    "        act_label  = char_id[chars[i]]\n",
    "        act_vec = char_mat[act_label].reshape(1,input_units)\n",
    "        out_vec = output['o'+str(i)]\n",
    "        \n",
    "        #output error\n",
    "        error_output[i-1] = out_vec - act_vec\n",
    "        \n",
    "        \n",
    "        #error_activation 'at'\n",
    "        eat = np.matmul(error_output[i-1],how.T)\n",
    "        #error_activation['ea' + str(i)] = np.matmul(error_output,how.T)\n",
    "        \n",
    "        #error_cell ct = zeros\n",
    "        ect = np.zeros(shape=eat.shape)\n",
    "        \n",
    "        #initialize error_lstm\n",
    "        error_lstm['elstm'+str(i)] = np.zeros([4,how.shape[0]])\n",
    "        \n",
    "        for j in range(i,0,-1):\n",
    "            elstm,ect,eat = calculate_errors(eat,ect,lstm_activation,cell_cache['c'+str(j)],cell_cache['c'+str(j-1)],parameters)\n",
    "            error_lstm['elstm'+str(j)] += elstm\n",
    "            #der = derivative(cache['a'+str(j)])\n",
    "            #error_activation['ea'+str(j)] = np.multiply(np.matmul(error_activation['ea'+str(j+1)],hhw.T),der)\n",
    "        \n",
    "        #sum the errors\n",
    "        #convert into mat to get derivatives\n",
    "        \n",
    "    #calculate derivatives\n",
    "    error_output = error_output.reshape(-1,how.shape[1])\n",
    "    #print(error_output.shape)\n",
    "    assert error_output.shape == (len(train_name)-1,how.shape[1])\n",
    "    \n",
    "    #activation matrix\n",
    "    activation_matrix = get_activation_matrix(activation_cache)\n",
    "    input_activation_matrix = get_input_activation_matrix(train_name,activation_cache,char_id,char_mat)\n",
    "    \n",
    "    \n",
    "    #derivative ouput\n",
    "    derivatives['dhow'] = np.matmul(activation_matrix.T,error_output)\n",
    "\n",
    "    for i in range(len(train_name)-1):\n",
    "        input_activation = input_activation_matrix[i].reshape(1,input_activation_matrix.shape[1])\n",
    "        elstm = error_lstm['elstm'+str(i+1)]\n",
    "        derivatives['dfgw'] += np.matmul(input_activation.T,elstm[0,:].reshape(1,elstm.shape[1]))\n",
    "        derivatives['digw'] += np.matmul(input_activation.T,elstm[1,:].reshape(1,elstm.shape[1]))\n",
    "        derivatives['dogw'] += np.matmul(input_activation.T,elstm[2,:].reshape(1,elstm.shape[1]))\n",
    "        derivatives['dggw'] += np.matmul(input_activation.T,elstm[3,:].reshape(1,elstm.shape[1]))\n",
    "        \n",
    "        #derivatives['dhhw'] += np.matmul(cache['a'+str(j-1)].T,error_activation['ea'+str(j)])\n",
    "        #in_vec = char_id[chars[j-1]]\n",
    "        #in_vec = char_mat[in_vec].reshape(1,input_units)\n",
    "        #derivatives['dihw'] += np.matmul(in_vec.T,error_activation['ea'+str(j)])\n",
    "    \n",
    "    derivatives['dfgw'] /= (len(train_name)-1)\n",
    "    derivatives['digw'] /= (len(train_name)-1)\n",
    "    derivatives['dogw'] /= (len(train_name)-1)\n",
    "    derivatives['dggw'] /= (len(train_name)-1)\n",
    "    derivatives['dhow'] /= (len(train_name)-1)\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters,derivatives,V):\n",
    "    \n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    how = parameters['how']\n",
    "    \n",
    "    dfgw = derivatives['dfgw']\n",
    "    digw = derivatives['digw']\n",
    "    dogw = derivatives['dogw']\n",
    "    dggw = derivatives['dggw']\n",
    "    dhow = derivatives['dhow']\n",
    "    \n",
    "    \n",
    "    Vfgw = V['Vfgw']\n",
    "    Vigw = V['Vigw']\n",
    "    Vogw = V['Vogw']\n",
    "    Vggw = V['Vggw']\n",
    "    Vhow = V['Vhow']\n",
    "    \n",
    "    Vfgw1 = beta*Vfgw + (1-beta)*dfgw\n",
    "    Vigw1 = beta*Vigw + (1-beta)*digw\n",
    "    Vogw1 = beta*Vogw + (1-beta)*dogw\n",
    "    Vggw1 = beta*Vggw + (1-beta)*dggw\n",
    "    Vhow1 = beta*Vhow + (1-beta)*dhow\n",
    "    \n",
    "    fgw = fgw - learning_rate*Vfgw1\n",
    "    igw = igw - learning_rate*Vigw1\n",
    "    ogw = ogw - learning_rate*Vogw1\n",
    "    ggw = ggw - learning_rate*Vggw1\n",
    "    how = how - learning_rate*Vhow1\n",
    "    \n",
    "    V['Vfgw'] = Vfgw1\n",
    "    V['Vigw'] = Vigw1\n",
    "    V['Vogw'] = Vogw1\n",
    "    V['Vggw'] = Vggw1\n",
    "    V['Vhow'] = Vhow1\n",
    "    \n",
    "    parameters['fgw'] = fgw\n",
    "    parameters['igw'] = igw\n",
    "    parameters['ogw'] = ogw\n",
    "    parameters['ggw'] = ggw\n",
    "    parameters['how'] = how\n",
    "    \n",
    "    return parameters,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_dataset,parameters,iters=1000):\n",
    "    parameters = initialize_parameters()\n",
    "    V = initialize_V(parameters)\n",
    "    print(parameters['fgw'].shape)\n",
    "    \n",
    "    \n",
    "    for step in range(iters):\n",
    "        total_loss = 0\n",
    "        total_perplexity = 0\n",
    "        total_acc = 0\n",
    "        for j in range(len(train_dataset)):\n",
    "            lstm_cache,activation_cache,cell_cache,output = forward_propagation(train_dataset[j,0],parameters,char_id,char_mat)\n",
    "            loss,perplexity,acc = cal_loss_accuracy(train_dataset[j,0],output,char_id,char_mat)\n",
    "            derivatives = backward_propagation(train_dataset[j,0],parameters,lstm_cache,activation_cache,cell_cache,output,char_id,char_mat)\n",
    "            total_loss += loss\n",
    "            total_perplexity += perplexity\n",
    "            total_acc += acc\n",
    "            \n",
    "        #derivatives['dfgw'] /= len(train_dataset)\n",
    "        #derivatives['digw'] /= len(train_dataset)\n",
    "        #derivatives['dogw'] /= len(train_dataset)\n",
    "        #derivatives['dggw'] /= len(train_dataset)\n",
    "        #derivatives['dhow'] /= len(train_dataset)\n",
    "\n",
    "            parameters,V = update_parameters(parameters,derivatives,V)\n",
    "        \n",
    "        total_loss /= len(train_dataset)\n",
    "        total_perplexity /= len(train_dataset)\n",
    "        total_acc /= len(train_dataset)\n",
    "        \n",
    "        if(step%200==0):\n",
    "            print('Step : {}'.format(step))\n",
    "            print(\"Loss : {}\".format(round(total_loss,2)))\n",
    "            print(\"Perp : {}\".format(round(total_perplexity,2)))\n",
    "            print(\"Accu : {}\".format(round(total_acc*100,2)))\n",
    "            print()\n",
    "    return parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
